{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Task- 8: Emotion Detection through Voice: Description: In this task, you will train a machine learning model to detect emotions using audio files. The model should be able to process both recorded voices and uploaded voice notes through a GUI. For example, if a voice note contains a happy emotion, the model should identify and label it as happy. The model should be designed to work exclusively with female voices; if a non-female voice is uploaded, it should prompt the user to upload a female voice instead. Guidelines: You should have a proper GUI with features for both upload voice notes and voice recording."
      ],
      "metadata": {
        "id": "MpD89fBeOM_G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "collapsed": true,
        "id": "9zgfxcwCxwte",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe91d325-d7ed-41ca-dee0-4e6909ffb2cb"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.25.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.8.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.5)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "pPAHE5EdS2Ki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0baedf1-dcb2-45db-f25d-e67e3117cfd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # Mount Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE0HqS3F2uW3"
      },
      "source": [
        "Emotion Detection Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "dm6Lu5yjTD6-"
      },
      "outputs": [],
      "source": [
        "#Step 2: Set Dataset Path\n",
        "\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define dataset path\n",
        "DATASET_PATH = \"/content/drive/MyDrive/RAVDESS_Dataset/audio_speech_actors_01-24\"\n",
        "data = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "r1W_3FK1TPRf"
      },
      "outputs": [],
      "source": [
        "#Step 3: Define Feature Extraction\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "# Function to extract features from an audio file\n",
        "def extract_features(file_path, augment=False):\n",
        "    y, sr = librosa.load(file_path, duration=3, offset=0.5)  # Load audio\n",
        "\n",
        "    # Apply augmentation if enabled\n",
        "    if augment:\n",
        "        y = add_noise(y)\n",
        "        y = pitch_shift(y, sr)\n",
        "        y = time_stretch(y)\n",
        "\n",
        "    # Extract features\n",
        "    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n",
        "    chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)\n",
        "    mel = np.mean(librosa.feature.melspectrogram(y=y, sr=sr).T, axis=0)\n",
        "\n",
        "    return np.hstack([mfcc, chroma, mel])  # Concatenate features\n",
        "\n",
        "# Function to add noise\n",
        "def add_noise(data, noise_factor=0.005):\n",
        "    noise = noise_factor * np.random.randn(len(data))\n",
        "    return data + noise\n",
        "\n",
        "# Function to shift pitch\n",
        "def pitch_shift(data, sr, n_steps=2):\n",
        "    return librosa.effects.pitch_shift(data, sr=sr, n_steps=n_steps)\n",
        "\n",
        "# Function to stretch time (speed up/down)\n",
        "def time_stretch(data, rate=0.9):\n",
        "    return librosa.effects.time_stretch(data, rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "h8eYstkqTW_Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45eefb4c-f0da-480a-cce3-f07f944ba87d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 576, Testing samples: 144\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Define RAVDESS dataset path\n",
        "#DATASET_PATH = \"/content/drive/MyDrive/RAVDESS_Dataset/audio_speech_actors_01-24\"\n",
        "\n",
        "# Emotion labels\n",
        "emotion_map = {\n",
        "    \"01\": \"neutral\", \"02\": \"calm\", \"03\": \"happy\", \"04\": \"sad\",\n",
        "    \"05\": \"angry\", \"06\": \"fearful\", \"07\": \"disgust\", \"08\": \"surprised\"\n",
        "}\n",
        "\n",
        "data = []\n",
        "\n",
        "# Load dataset\n",
        "for folder in os.listdir(DATASET_PATH):\n",
        "    folder_path = os.path.join(DATASET_PATH, folder)\n",
        "    if not os.path.isdir(folder_path):\n",
        "        continue\n",
        "\n",
        "    for file in os.listdir(folder_path):\n",
        "        if file.endswith(\".wav\"):\n",
        "            parts = file.split(\"-\")\n",
        "            emotion = emotion_map.get(parts[2], \"unknown\")  # Extract emotion\n",
        "            actor_id = ''.join(filter(str.isdigit, parts[-1].split(\".\")[0]))  # Keep only digits\n",
        "            actor_id = int(actor_id) if actor_id.isdigit() else None  # Convert to integer safely\n",
        "\n",
        "\n",
        "            # Use only female voices (Even Actor IDs are Female)\n",
        "            if actor_id % 2 == 0:\n",
        "                file_path = os.path.join(folder_path, file)\n",
        "                feature_vector = extract_features(file_path)\n",
        "                data.append([feature_vector, emotion])\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data, columns=[\"features\", \"emotion\"])\n",
        "df[\"emotion\"] = df[\"emotion\"].astype(\"category\").cat.codes  # Encode labels\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array(df[\"features\"].tolist())\n",
        "y = np.array(df[\"emotion\"].tolist())\n",
        "\n",
        "# One-hot encode labels\n",
        "y = to_categorical(y, num_classes=8)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape for CNN\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "print(f\"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "collapsed": true,
        "id": "3NxB3-QlWepb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "336faf1a-f55e-4c9b-893c-cbc60c29fb3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.1217 - loss: 5.0859 - val_accuracy: 0.2361 - val_loss: 1.9398\n",
            "Epoch 2/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.2487 - loss: 2.1933 - val_accuracy: 0.2292 - val_loss: 1.9021\n",
            "Epoch 3/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.2291 - loss: 1.9952 - val_accuracy: 0.2361 - val_loss: 1.8809\n",
            "Epoch 4/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.2215 - loss: 1.9105 - val_accuracy: 0.2847 - val_loss: 1.8379\n",
            "Epoch 5/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.3014 - loss: 1.8064 - val_accuracy: 0.2500 - val_loss: 1.7823\n",
            "Epoch 6/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.2766 - loss: 1.7904 - val_accuracy: 0.3403 - val_loss: 1.8877\n",
            "Epoch 7/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.3301 - loss: 1.7424 - val_accuracy: 0.4097 - val_loss: 1.7678\n",
            "Epoch 8/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.3263 - loss: 1.6982 - val_accuracy: 0.3542 - val_loss: 1.9409\n",
            "Epoch 9/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.3645 - loss: 1.7072 - val_accuracy: 0.4028 - val_loss: 1.7282\n",
            "Epoch 10/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.3926 - loss: 1.5828 - val_accuracy: 0.4653 - val_loss: 1.6655\n",
            "Epoch 11/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.4130 - loss: 1.5519 - val_accuracy: 0.4583 - val_loss: 1.6343\n",
            "Epoch 12/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.3977 - loss: 1.5307 - val_accuracy: 0.4167 - val_loss: 1.5113\n",
            "Epoch 13/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.4082 - loss: 1.5213 - val_accuracy: 0.4444 - val_loss: 1.5613\n",
            "Epoch 14/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.3980 - loss: 1.5160 - val_accuracy: 0.4514 - val_loss: 1.9717\n",
            "Epoch 15/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 0.4408 - loss: 1.4682 - val_accuracy: 0.4792 - val_loss: 1.8517\n",
            "Epoch 16/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.4330 - loss: 1.4379 - val_accuracy: 0.4583 - val_loss: 1.6790\n",
            "Epoch 17/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.4622 - loss: 1.3209 - val_accuracy: 0.4375 - val_loss: 1.4821\n",
            "Epoch 18/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.4724 - loss: 1.3658 - val_accuracy: 0.4583 - val_loss: 1.6299\n",
            "Epoch 19/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.4881 - loss: 1.3264 - val_accuracy: 0.4722 - val_loss: 1.8703\n",
            "Epoch 20/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.4501 - loss: 1.4072 - val_accuracy: 0.4722 - val_loss: 1.9937\n",
            "Epoch 21/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.4939 - loss: 1.3414 - val_accuracy: 0.4653 - val_loss: 1.9448\n",
            "Epoch 22/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.5262 - loss: 1.3097 - val_accuracy: 0.4722 - val_loss: 1.7650\n",
            "Epoch 23/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.5371 - loss: 1.3278 - val_accuracy: 0.5000 - val_loss: 1.6209\n",
            "Epoch 24/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.5305 - loss: 1.2474 - val_accuracy: 0.4583 - val_loss: 1.6015\n",
            "Epoch 25/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.5153 - loss: 1.2473 - val_accuracy: 0.4792 - val_loss: 1.5087\n",
            "Epoch 26/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.5562 - loss: 1.2162 - val_accuracy: 0.5486 - val_loss: 1.4610\n",
            "Epoch 27/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.5387 - loss: 1.1603 - val_accuracy: 0.5000 - val_loss: 1.5075\n",
            "Epoch 28/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5455 - loss: 1.1940 - val_accuracy: 0.5486 - val_loss: 1.5154\n",
            "Epoch 29/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.5953 - loss: 1.1141 - val_accuracy: 0.4861 - val_loss: 1.6451\n",
            "Epoch 30/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.5556 - loss: 1.1564 - val_accuracy: 0.5069 - val_loss: 1.5337\n",
            "Epoch 31/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.5544 - loss: 1.1892 - val_accuracy: 0.5069 - val_loss: 1.4956\n",
            "Epoch 32/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.5910 - loss: 1.1096 - val_accuracy: 0.5278 - val_loss: 1.4523\n",
            "Epoch 33/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.5288 - loss: 1.1362 - val_accuracy: 0.5139 - val_loss: 1.5584\n",
            "Epoch 34/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.5661 - loss: 1.1263 - val_accuracy: 0.5694 - val_loss: 1.5874\n",
            "Epoch 35/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.6045 - loss: 1.0531 - val_accuracy: 0.5069 - val_loss: 1.8584\n",
            "Epoch 36/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.6434 - loss: 1.0136 - val_accuracy: 0.4931 - val_loss: 1.4373\n",
            "Epoch 37/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.6169 - loss: 1.0401 - val_accuracy: 0.5694 - val_loss: 1.3075\n",
            "Epoch 38/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.5673 - loss: 1.1337 - val_accuracy: 0.5556 - val_loss: 1.5096\n",
            "Epoch 39/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.5978 - loss: 1.0201 - val_accuracy: 0.5278 - val_loss: 2.0324\n",
            "Epoch 40/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.6132 - loss: 0.9965 - val_accuracy: 0.5625 - val_loss: 1.9011\n",
            "Epoch 41/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.6225 - loss: 0.9763 - val_accuracy: 0.5625 - val_loss: 1.8633\n",
            "Epoch 42/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.6317 - loss: 1.0008 - val_accuracy: 0.5278 - val_loss: 1.8148\n",
            "Epoch 43/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.6357 - loss: 0.9456 - val_accuracy: 0.5625 - val_loss: 1.8435\n",
            "Epoch 44/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.6270 - loss: 0.9498 - val_accuracy: 0.5625 - val_loss: 1.9891\n",
            "Epoch 45/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.6360 - loss: 0.9756 - val_accuracy: 0.5694 - val_loss: 1.9796\n",
            "Epoch 46/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.6528 - loss: 0.9151 - val_accuracy: 0.5556 - val_loss: 1.9226\n",
            "Epoch 47/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6645 - loss: 0.9317 - val_accuracy: 0.5833 - val_loss: 2.0023\n",
            "Epoch 48/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.6852 - loss: 0.8528 - val_accuracy: 0.5625 - val_loss: 2.1222\n",
            "Epoch 49/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.6504 - loss: 0.9611 - val_accuracy: 0.5625 - val_loss: 1.8066\n",
            "Epoch 50/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.6378 - loss: 0.9120 - val_accuracy: 0.5625 - val_loss: 1.6380\n",
            "Epoch 51/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6539 - loss: 0.8882 - val_accuracy: 0.5764 - val_loss: 1.6055\n",
            "Epoch 52/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.6850 - loss: 0.8068 - val_accuracy: 0.5556 - val_loss: 1.6200\n",
            "Epoch 53/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - accuracy: 0.6858 - loss: 0.8043 - val_accuracy: 0.5694 - val_loss: 1.6079\n",
            "Epoch 54/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - accuracy: 0.6714 - loss: 0.8491 - val_accuracy: 0.5694 - val_loss: 1.6703\n",
            "Epoch 55/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.6770 - loss: 0.8693 - val_accuracy: 0.5764 - val_loss: 1.7942\n",
            "Epoch 56/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7055 - loss: 0.7603 - val_accuracy: 0.5417 - val_loss: 1.8866\n",
            "Epoch 57/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.6726 - loss: 0.8186 - val_accuracy: 0.5625 - val_loss: 1.7088\n",
            "Epoch 58/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7098 - loss: 0.8201 - val_accuracy: 0.5903 - val_loss: 1.5025\n",
            "Epoch 59/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6879 - loss: 0.8384 - val_accuracy: 0.5556 - val_loss: 1.4991\n",
            "Epoch 60/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7376 - loss: 0.7516 - val_accuracy: 0.5764 - val_loss: 1.5541\n",
            "Epoch 61/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7429 - loss: 0.7071 - val_accuracy: 0.5833 - val_loss: 1.5520\n",
            "Epoch 62/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.6913 - loss: 0.7680 - val_accuracy: 0.6042 - val_loss: 1.7835\n",
            "Epoch 63/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7182 - loss: 0.7140 - val_accuracy: 0.5833 - val_loss: 1.9413\n",
            "Epoch 64/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.7679 - loss: 0.6667 - val_accuracy: 0.5694 - val_loss: 1.9660\n",
            "Epoch 65/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7417 - loss: 0.7060 - val_accuracy: 0.5764 - val_loss: 1.8028\n",
            "Epoch 66/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.7313 - loss: 0.7439 - val_accuracy: 0.5764 - val_loss: 1.7591\n",
            "Epoch 67/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.7348 - loss: 0.7133 - val_accuracy: 0.6111 - val_loss: 1.6454\n",
            "Epoch 68/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.7611 - loss: 0.6555 - val_accuracy: 0.5833 - val_loss: 1.4469\n",
            "Epoch 69/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7584 - loss: 0.6055 - val_accuracy: 0.5972 - val_loss: 1.4412\n",
            "Epoch 70/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7749 - loss: 0.6106 - val_accuracy: 0.6250 - val_loss: 1.4618\n",
            "Epoch 71/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7703 - loss: 0.6369 - val_accuracy: 0.5903 - val_loss: 1.3807\n",
            "Epoch 72/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.7427 - loss: 0.6857 - val_accuracy: 0.6042 - val_loss: 1.5138\n",
            "Epoch 73/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - accuracy: 0.7586 - loss: 0.6722 - val_accuracy: 0.6111 - val_loss: 1.6649\n",
            "Epoch 74/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - accuracy: 0.7840 - loss: 0.5952 - val_accuracy: 0.5694 - val_loss: 1.6124\n",
            "Epoch 75/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - accuracy: 0.7833 - loss: 0.5700 - val_accuracy: 0.5764 - val_loss: 1.9789\n",
            "Epoch 76/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.7645 - loss: 0.6129 - val_accuracy: 0.5556 - val_loss: 2.3896\n",
            "Epoch 77/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7666 - loss: 0.6187 - val_accuracy: 0.6042 - val_loss: 2.4294\n",
            "Epoch 78/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7987 - loss: 0.5988 - val_accuracy: 0.5903 - val_loss: 2.1365\n",
            "Epoch 79/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7897 - loss: 0.5808 - val_accuracy: 0.5694 - val_loss: 2.3770\n",
            "Epoch 80/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.7852 - loss: 0.5872 - val_accuracy: 0.5764 - val_loss: 1.7928\n",
            "Epoch 81/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7564 - loss: 0.6465 - val_accuracy: 0.6458 - val_loss: 1.5638\n",
            "Epoch 82/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7695 - loss: 0.5971 - val_accuracy: 0.6319 - val_loss: 1.5355\n",
            "Epoch 83/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8097 - loss: 0.5087 - val_accuracy: 0.5764 - val_loss: 2.0544\n",
            "Epoch 84/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8119 - loss: 0.5934 - val_accuracy: 0.5903 - val_loss: 2.4928\n",
            "Epoch 85/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8312 - loss: 0.4945 - val_accuracy: 0.6389 - val_loss: 2.2573\n",
            "Epoch 86/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7908 - loss: 0.5789 - val_accuracy: 0.6736 - val_loss: 2.3207\n",
            "Epoch 87/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8261 - loss: 0.5280 - val_accuracy: 0.6181 - val_loss: 2.0020\n",
            "Epoch 88/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8021 - loss: 0.5308 - val_accuracy: 0.6250 - val_loss: 1.1483\n",
            "Epoch 89/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.7990 - loss: 0.5797 - val_accuracy: 0.6319 - val_loss: 1.0850\n",
            "Epoch 90/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7994 - loss: 0.5296 - val_accuracy: 0.6181 - val_loss: 1.2409\n",
            "Epoch 91/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8191 - loss: 0.5033 - val_accuracy: 0.6042 - val_loss: 1.2833\n",
            "Epoch 92/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7977 - loss: 0.5320 - val_accuracy: 0.6181 - val_loss: 1.3279\n",
            "Epoch 93/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.8453 - loss: 0.4472 - val_accuracy: 0.6250 - val_loss: 1.2194\n",
            "Epoch 94/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - accuracy: 0.8219 - loss: 0.4927 - val_accuracy: 0.6528 - val_loss: 1.2305\n",
            "Epoch 95/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 0.8448 - loss: 0.4726 - val_accuracy: 0.6458 - val_loss: 1.2468\n",
            "Epoch 96/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 0.8169 - loss: 0.4596 - val_accuracy: 0.6111 - val_loss: 1.2692\n",
            "Epoch 97/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.8452 - loss: 0.4880 - val_accuracy: 0.6250 - val_loss: 1.2415\n",
            "Epoch 98/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.8342 - loss: 0.4567 - val_accuracy: 0.6528 - val_loss: 1.5868\n",
            "Epoch 99/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.8286 - loss: 0.4339 - val_accuracy: 0.6458 - val_loss: 2.4095\n",
            "Epoch 100/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8461 - loss: 0.4189 - val_accuracy: 0.6042 - val_loss: 2.3523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model training complete and saved to Google Drive!\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "\n",
        "# CNN Model\n",
        "model = Sequential([\n",
        "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Conv1D(128, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(8, activation='softmax')  # 8 emotion classes\n",
        "])\n",
        "\n",
        "# Compile Model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train Model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test))\n",
        "\n",
        "# Save Model\n",
        "model.save(\"/content/drive/MyDrive/emotion_detection_model.h5\")\n",
        "print(\"✅ Model training complete and saved to Google Drive!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Mq8XXqUmWmqf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d4bc24f-2204-4f57-f00a-a0bc34ed3f35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5577 - loss: 2.2681\n",
            "New Test Accuracy: 60.42%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate Model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"New Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "AD7C0ktHp3UC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4e3a81e-3bc0-4458-ee84-6efb075074c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded 1450 samples successfully.\n",
            "Epoch 1/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 138ms/step - accuracy: 0.7164 - loss: 0.5771 - val_accuracy: 0.8517 - val_loss: 0.3812\n",
            "Epoch 2/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - accuracy: 0.8463 - loss: 0.3712 - val_accuracy: 0.8483 - val_loss: 0.3612\n",
            "Epoch 3/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 167ms/step - accuracy: 0.8309 - loss: 0.4085 - val_accuracy: 0.8552 - val_loss: 0.3738\n",
            "Epoch 4/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - accuracy: 0.8308 - loss: 0.3860 - val_accuracy: 0.8724 - val_loss: 0.3286\n",
            "Epoch 5/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 152ms/step - accuracy: 0.8561 - loss: 0.3530 - val_accuracy: 0.8586 - val_loss: 0.3637\n",
            "Epoch 6/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 127ms/step - accuracy: 0.8663 - loss: 0.3889 - val_accuracy: 0.8724 - val_loss: 0.3375\n",
            "Epoch 7/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 180ms/step - accuracy: 0.8493 - loss: 0.3628 - val_accuracy: 0.8759 - val_loss: 0.3180\n",
            "Epoch 8/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 117ms/step - accuracy: 0.8660 - loss: 0.3404 - val_accuracy: 0.8793 - val_loss: 0.3575\n",
            "Epoch 9/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 155ms/step - accuracy: 0.8566 - loss: 0.4007 - val_accuracy: 0.8414 - val_loss: 0.3752\n",
            "Epoch 10/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 114ms/step - accuracy: 0.8653 - loss: 0.3538 - val_accuracy: 0.8483 - val_loss: 0.3429\n",
            "Epoch 11/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - accuracy: 0.8317 - loss: 0.4457 - val_accuracy: 0.8241 - val_loss: 0.3852\n",
            "Epoch 12/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 140ms/step - accuracy: 0.8713 - loss: 0.3484 - val_accuracy: 0.8483 - val_loss: 0.3516\n",
            "✅ Gender classification model saved!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "\n",
        "# ----------------------------------------\n",
        "# FEATURE EXTRACTION FUNCTION\n",
        "# ----------------------------------------\n",
        "def extract_features(file_path):\n",
        "    y, sr = librosa.load(file_path, duration=3, offset=0.5)\n",
        "\n",
        "    if len(y) < 1024:\n",
        "        raise ValueError(\"Audio too short for n_fft\")\n",
        "\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
        "\n",
        "    features = np.hstack([\n",
        "        np.mean(mfcc, axis=1),\n",
        "        np.mean(chroma, axis=1),\n",
        "        np.mean(contrast, axis=1),\n",
        "        np.mean(tonnetz, axis=1)\n",
        "    ])\n",
        "    return features\n",
        "\n",
        "# ----------------------------------------\n",
        "# LOAD DATA\n",
        "# ----------------------------------------\n",
        "data_dir = '/content/drive/MyDrive/RAVDESS_Dataset/audio_speech_actors_01-24'\n",
        "X, y = [], []\n",
        "\n",
        "for folder in os.listdir(data_dir):\n",
        "    folder_path = os.path.join(data_dir, folder)\n",
        "    if not os.path.isdir(folder_path):\n",
        "        continue\n",
        "\n",
        "    for file in os.listdir(folder_path):\n",
        "        if file.endswith('.wav'):\n",
        "            file_path = os.path.join(folder_path, file)\n",
        "            try:\n",
        "                features = extract_features(file_path)\n",
        "                X.append(features)\n",
        "\n",
        "                # Extract actor ID using regex and handle cases like \"17 (1)\"\n",
        "                cleaned_filename = re.sub(r'\\s*\\(.*\\)', '', file)  # Remove (1), (2), etc.\n",
        "                actor_id = int(cleaned_filename.split('-')[-1].split('.')[0])\n",
        "\n",
        "                label = 0 if actor_id % 2 == 1 else 1  # 0 = Male, 1 = Female\n",
        "                y.append(label)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Skipping {file}: {e}\")\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(f\"✅ Loaded {len(X)} samples successfully.\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# NORMALIZE & RESHAPE FEATURES\n",
        "# ----------------------------------------\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X = np.expand_dims(X, axis=2)  # Shape: (samples, timesteps, features)\n",
        "\n",
        "# ----------------------------------------\n",
        "# TRAIN/TEST SPLIT\n",
        "# ----------------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ----------------------------------------\n",
        "# BUILD LSTM MODEL\n",
        "# ----------------------------------------\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(128, return_sequences=True), input_shape=(X.shape[1], 1)),\n",
        "    Dropout(0.3),\n",
        "    LSTM(64),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# ----------------------------------------\n",
        "# COMPILE & TRAIN MODEL\n",
        "# ----------------------------------------\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# SAVE MODEL\n",
        "# ----------------------------------------\n",
        "model.save(\"gender_classification_model.keras\")\n",
        "print(\"✅ Gender classification model saved!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "Qir1QJC21TT9",
        "outputId": "799c84d1-e83f-42b6-be73-29804bd59c50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://56e8b1010dab4447dd.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://56e8b1010dab4447dd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import librosa\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "\n",
        "# Load models\n",
        "emotion_model = load_model(\"/content/drive/MyDrive/emotion_detection_model.h5\")\n",
        "gender_model = load_model(\"/content/gender_classification_model.keras\")\n",
        "\n",
        "\n",
        "# Emotion labels\n",
        "emotion_labels = {\n",
        "    0: \"neutral\", 1: \"calm\", 2: \"happy\", 3: \"sad\",\n",
        "    4: \"angry\", 5: \"fearful\", 6: \"disgust\", 7: \"surprised\"\n",
        "}\n",
        "\n",
        "# Feature extractor for both models\n",
        "def extract_features(file_path):\n",
        "    y, sr = librosa.load(file_path, duration=3, offset=0.5)\n",
        "    if len(y) < 1024:\n",
        "        raise ValueError(\"Audio too short\")\n",
        "\n",
        "    # Features for gender model\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
        "\n",
        "    gender_features = np.hstack([\n",
        "        np.mean(mfcc, axis=1),\n",
        "        np.mean(chroma, axis=1),\n",
        "        np.mean(contrast, axis=1),\n",
        "        np.mean(tonnetz, axis=1)\n",
        "    ])\n",
        "\n",
        "    # Features for emotion model\n",
        "    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n",
        "    chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)\n",
        "    mel = np.mean(librosa.feature.melspectrogram(y=y, sr=sr).T, axis=0)\n",
        "    emotion_features = np.hstack([mfcc, chroma, mel])\n",
        "\n",
        "    return gender_features, emotion_features\n",
        "\n",
        "# Prediction function\n",
        "def predict_audio(audio_path):\n",
        "    try:\n",
        "        gender_feat, emotion_feat = extract_features(audio_path)\n",
        "\n",
        "        # Gender prediction\n",
        "        gender_scaled = scaler.transform([gender_feat])\n",
        "        gender_input = np.expand_dims(gender_scaled, axis=2)\n",
        "        gender_prob = gender_model.predict(gender_input)[0][0]\n",
        "        gender = \"Female\" if gender_prob >= 0.5 else \"Male\"\n",
        "\n",
        "        if gender == \"Male\":\n",
        "            return f\"🚫 Detected Gender: Male (Confidence: {1 - gender_prob:.2f})\\nPlease upload a female voice.\"\n",
        "\n",
        "        # Emotion prediction\n",
        "        emotion_input = np.expand_dims(emotion_feat, axis=0)\n",
        "        emotion_input = np.expand_dims(emotion_input, axis=2)\n",
        "        emotion_pred = emotion_model.predict(emotion_input)\n",
        "        emotion_label = emotion_labels[np.argmax(emotion_pred)]\n",
        "        emotion_conf = np.max(emotion_pred)\n",
        "\n",
        "        return (\n",
        "            f\"✅ Detected Gender: Female (Confidence: {gender_prob:.2f})\\n\"\n",
        "            f\"🎭 Emotion: {emotion_label.capitalize()} (Confidence: {emotion_conf:.2f})\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error processing audio: {str(e)}\"\n",
        "\n",
        "# Gradio Interface\n",
        "interface = gr.Interface(\n",
        "    fn=predict_audio,\n",
        "    inputs=gr.Audio(type=\"filepath\", label=\"Upload or Record Voice\"),\n",
        "    outputs=gr.Textbox(label=\"Prediction\"),\n",
        "    title=\"🎙️ Emotion & Gender Detection (Female Voices Only)\",\n",
        "    description=\"Upload or record a voice clip. System detects gender first — only female voices are accepted for emotion prediction.\"\n",
        ")\n",
        "\n",
        "interface.launch(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pECRwaNb1T1U"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}