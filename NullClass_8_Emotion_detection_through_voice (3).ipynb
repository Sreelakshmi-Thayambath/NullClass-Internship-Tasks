{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9zgfxcwCxwte",
        "outputId": "40020e11-6766-4ecd-9eb9-f7398d8b7618",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.25.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/safehttpx/\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.1)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.25.0-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.25.0 gradio-client-1.8.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.5 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPAHE5EdS2Ki",
        "outputId": "fce02bf0-205a-4c05-ad6d-b0bb79e64fee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # Mount Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Emotion Detection Model"
      ],
      "metadata": {
        "id": "PE0HqS3F2uW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Set Dataset Path\n",
        "\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define dataset path (Modify the path if needed)\n",
        "DATASET_PATH = \"/content/drive/MyDrive/RAVDESS_Dataset/audio_speech_actors_01-24\"\n",
        "data = []\n"
      ],
      "metadata": {
        "id": "dm6Lu5yjTD6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Define Feature Extraction\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "# Function to extract features from an audio file\n",
        "def extract_features(file_path, augment=False):\n",
        "    y, sr = librosa.load(file_path, duration=3, offset=0.5)  # Load audio\n",
        "\n",
        "    # Apply augmentation if enabled\n",
        "    if augment:\n",
        "        y = add_noise(y)\n",
        "        y = pitch_shift(y, sr)\n",
        "        y = time_stretch(y)\n",
        "\n",
        "    # Extract features\n",
        "    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n",
        "    chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)\n",
        "    mel = np.mean(librosa.feature.melspectrogram(y=y, sr=sr).T, axis=0)\n",
        "\n",
        "    return np.hstack([mfcc, chroma, mel])  # Concatenate features\n",
        "\n",
        "# Function to add noise\n",
        "def add_noise(data, noise_factor=0.005):\n",
        "    noise = noise_factor * np.random.randn(len(data))\n",
        "    return data + noise\n",
        "\n",
        "# Function to shift pitch\n",
        "def pitch_shift(data, sr, n_steps=2):\n",
        "    return librosa.effects.pitch_shift(data, sr=sr, n_steps=n_steps)\n",
        "\n",
        "# Function to stretch time (speed up/down)\n",
        "def time_stretch(data, rate=0.9):\n",
        "    return librosa.effects.time_stretch(data, rate)\n"
      ],
      "metadata": {
        "id": "r1W_3FK1TPRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Define RAVDESS dataset path\n",
        "#DATASET_PATH = \"/content/drive/MyDrive/RAVDESS_Dataset/audio_speech_actors_01-24\"\n",
        "\n",
        "# Emotion labels\n",
        "emotion_map = {\n",
        "    \"01\": \"neutral\", \"02\": \"calm\", \"03\": \"happy\", \"04\": \"sad\",\n",
        "    \"05\": \"angry\", \"06\": \"fearful\", \"07\": \"disgust\", \"08\": \"surprised\"\n",
        "}\n",
        "\n",
        "data = []\n",
        "\n",
        "# Load dataset\n",
        "for folder in os.listdir(DATASET_PATH):\n",
        "    folder_path = os.path.join(DATASET_PATH, folder)\n",
        "    if not os.path.isdir(folder_path):\n",
        "        continue\n",
        "\n",
        "    for file in os.listdir(folder_path):\n",
        "        if file.endswith(\".wav\"):\n",
        "            parts = file.split(\"-\")\n",
        "            emotion = emotion_map.get(parts[2], \"unknown\")  # Extract emotion\n",
        "            actor_id = ''.join(filter(str.isdigit, parts[-1].split(\".\")[0]))  # Keep only digits\n",
        "            actor_id = int(actor_id) if actor_id.isdigit() else None  # Convert to integer safely\n",
        "\n",
        "\n",
        "            # Use only female voices (Even Actor IDs are Female)\n",
        "            if actor_id % 2 == 0:\n",
        "                file_path = os.path.join(folder_path, file)\n",
        "                feature_vector = extract_features(file_path)\n",
        "                data.append([feature_vector, emotion])\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data, columns=[\"features\", \"emotion\"])\n",
        "df[\"emotion\"] = df[\"emotion\"].astype(\"category\").cat.codes  # Encode labels\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array(df[\"features\"].tolist())\n",
        "y = np.array(df[\"emotion\"].tolist())\n",
        "\n",
        "# One-hot encode labels\n",
        "y = to_categorical(y, num_classes=8)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape for CNN\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "print(f\"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}\")\n"
      ],
      "metadata": {
        "id": "h8eYstkqTW_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "\n",
        "# CNN Model\n",
        "model = Sequential([\n",
        "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Conv1D(128, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(8, activation='softmax')  # 8 emotion classes\n",
        "])\n",
        "\n",
        "# Compile Model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train Model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test))\n",
        "\n",
        "# Save Model\n",
        "model.save(\"/content/drive/MyDrive/emotion_detection_model.h5\")\n",
        "print(\"✅ Model training complete and saved to Google Drive!\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3NxB3-QlWepb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"New Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Mq8XXqUmWmqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "\n",
        "# ----------------------------------------\n",
        "# FEATURE EXTRACTION FUNCTION\n",
        "# ----------------------------------------\n",
        "def extract_features(file_path):\n",
        "    y, sr = librosa.load(file_path, duration=3, offset=0.5)\n",
        "\n",
        "    if len(y) < 1024:\n",
        "        raise ValueError(\"Audio too short for n_fft\")\n",
        "\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
        "\n",
        "    features = np.hstack([\n",
        "        np.mean(mfcc, axis=1),\n",
        "        np.mean(chroma, axis=1),\n",
        "        np.mean(contrast, axis=1),\n",
        "        np.mean(tonnetz, axis=1)\n",
        "    ])\n",
        "    return features\n",
        "\n",
        "# ----------------------------------------\n",
        "# LOAD DATA\n",
        "# ----------------------------------------\n",
        "data_dir = '/content/drive/MyDrive/RAVDESS_Dataset/audio_speech_actors_01-24'\n",
        "X, y = [], []\n",
        "\n",
        "for folder in os.listdir(data_dir):\n",
        "    folder_path = os.path.join(data_dir, folder)\n",
        "    if not os.path.isdir(folder_path):\n",
        "        continue\n",
        "\n",
        "    for file in os.listdir(folder_path):\n",
        "        if file.endswith('.wav'):\n",
        "            file_path = os.path.join(folder_path, file)\n",
        "            try:\n",
        "                features = extract_features(file_path)\n",
        "                X.append(features)\n",
        "\n",
        "                # Extract actor ID using regex and handle cases like \"17 (1)\"\n",
        "                cleaned_filename = re.sub(r'\\s*\\(.*\\)', '', file)  # Remove (1), (2), etc.\n",
        "                actor_id = int(cleaned_filename.split('-')[-1].split('.')[0])\n",
        "\n",
        "                label = 0 if actor_id % 2 == 1 else 1  # 0 = Male, 1 = Female\n",
        "                y.append(label)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Skipping {file}: {e}\")\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(f\"✅ Loaded {len(X)} samples successfully.\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# NORMALIZE & RESHAPE FEATURES\n",
        "# ----------------------------------------\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X = np.expand_dims(X, axis=2)  # Shape: (samples, timesteps, features)\n",
        "\n",
        "# ----------------------------------------\n",
        "# TRAIN/TEST SPLIT\n",
        "# ----------------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ----------------------------------------\n",
        "# BUILD LSTM MODEL\n",
        "# ----------------------------------------\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(128, return_sequences=True), input_shape=(X.shape[1], 1)),\n",
        "    Dropout(0.3),\n",
        "    LSTM(64),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# ----------------------------------------\n",
        "# COMPILE & TRAIN MODEL\n",
        "# ----------------------------------------\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# SAVE MODEL\n",
        "# ----------------------------------------\n",
        "model.save(\"gender_classification_model.keras\")\n",
        "print(\"✅ Gender classification model saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD7C0ktHp3UC",
        "outputId": "167fa408-45e5-4c90-ec6f-c8af17dd1c97"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=863\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=874\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=840\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=851\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded 1450 samples successfully.\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 124ms/step - accuracy: 0.7564 - loss: 0.5643 - val_accuracy: 0.8207 - val_loss: 0.4524\n",
            "Epoch 2/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 150ms/step - accuracy: 0.8555 - loss: 0.4255 - val_accuracy: 0.8379 - val_loss: 0.4472\n",
            "Epoch 3/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 110ms/step - accuracy: 0.8506 - loss: 0.4312 - val_accuracy: 0.8379 - val_loss: 0.4599\n",
            "Epoch 4/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 163ms/step - accuracy: 0.8056 - loss: 0.4744 - val_accuracy: 0.8276 - val_loss: 0.4168\n",
            "Epoch 5/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 110ms/step - accuracy: 0.8190 - loss: 0.4271 - val_accuracy: 0.8724 - val_loss: 0.3712\n",
            "Epoch 6/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 168ms/step - accuracy: 0.8353 - loss: 0.3814 - val_accuracy: 0.8483 - val_loss: 0.4061\n",
            "Epoch 7/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 110ms/step - accuracy: 0.8347 - loss: 0.4128 - val_accuracy: 0.8207 - val_loss: 0.4374\n",
            "Epoch 8/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 171ms/step - accuracy: 0.8473 - loss: 0.4057 - val_accuracy: 0.8690 - val_loss: 0.3751\n",
            "Epoch 9/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 112ms/step - accuracy: 0.8345 - loss: 0.4096 - val_accuracy: 0.8517 - val_loss: 0.4006\n",
            "Epoch 10/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 114ms/step - accuracy: 0.8595 - loss: 0.3688 - val_accuracy: 0.8655 - val_loss: 0.3658\n",
            "Epoch 11/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 165ms/step - accuracy: 0.8439 - loss: 0.3837 - val_accuracy: 0.8586 - val_loss: 0.3963\n",
            "Epoch 12/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 133ms/step - accuracy: 0.8704 - loss: 0.3811 - val_accuracy: 0.8862 - val_loss: 0.3474\n",
            "Epoch 13/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 134ms/step - accuracy: 0.8783 - loss: 0.3759 - val_accuracy: 0.8517 - val_loss: 0.4032\n",
            "Epoch 14/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 112ms/step - accuracy: 0.8556 - loss: 0.3693 - val_accuracy: 0.8517 - val_loss: 0.4030\n",
            "Epoch 15/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 172ms/step - accuracy: 0.8789 - loss: 0.3540 - val_accuracy: 0.8310 - val_loss: 0.4446\n",
            "Epoch 16/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 109ms/step - accuracy: 0.8743 - loss: 0.3721 - val_accuracy: 0.8276 - val_loss: 0.4422\n",
            "Epoch 17/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 169ms/step - accuracy: 0.8731 - loss: 0.3440 - val_accuracy: 0.8241 - val_loss: 0.4319\n",
            "✅ Gender classification model saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import librosa\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "\n",
        "# Load models\n",
        "emotion_model = load_model(\"/content/drive/MyDrive/emotion_detection_model.h5\")\n",
        "gender_model = load_model(\"/content/gender_classification_model.keras\")\n",
        "\n",
        "# Load scaler used during gender model training\n",
        "# If you used a saved scaler, load it like this:\n",
        "# scaler = pickle.load(open(\"scaler.pkl\", \"rb\"))\n",
        "# But if you're using the same runtime, reuse the same `scaler` object from above\n",
        "\n",
        "# If not saved, you can re-fit using the same training data used for gender model (not shown here)\n",
        "\n",
        "# Emotion labels\n",
        "emotion_labels = {\n",
        "    0: \"neutral\", 1: \"calm\", 2: \"happy\", 3: \"sad\",\n",
        "    4: \"angry\", 5: \"fearful\", 6: \"disgust\", 7: \"surprised\"\n",
        "}\n",
        "\n",
        "# Feature extractor for both models\n",
        "def extract_features(file_path):\n",
        "    y, sr = librosa.load(file_path, duration=3, offset=0.5)\n",
        "    if len(y) < 1024:\n",
        "        raise ValueError(\"Audio too short\")\n",
        "\n",
        "    # Features for gender model\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
        "\n",
        "    gender_features = np.hstack([\n",
        "        np.mean(mfcc, axis=1),\n",
        "        np.mean(chroma, axis=1),\n",
        "        np.mean(contrast, axis=1),\n",
        "        np.mean(tonnetz, axis=1)\n",
        "    ])\n",
        "\n",
        "    # Features for emotion model\n",
        "    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n",
        "    chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)\n",
        "    mel = np.mean(librosa.feature.melspectrogram(y=y, sr=sr).T, axis=0)\n",
        "    emotion_features = np.hstack([mfcc, chroma, mel])\n",
        "\n",
        "    return gender_features, emotion_features\n",
        "\n",
        "# Prediction function\n",
        "def predict_audio(audio_path):\n",
        "    try:\n",
        "        gender_feat, emotion_feat = extract_features(audio_path)\n",
        "\n",
        "        # Gender prediction\n",
        "        gender_scaled = scaler.transform([gender_feat])\n",
        "        gender_input = np.expand_dims(gender_scaled, axis=2)\n",
        "        gender_prob = gender_model.predict(gender_input)[0][0]\n",
        "        gender = \"Female\" if gender_prob >= 0.5 else \"Male\"\n",
        "\n",
        "        if gender == \"Male\":\n",
        "            return f\"🚫 Detected Gender: Male (Confidence: {1 - gender_prob:.2f})\\nPlease upload a female voice.\"\n",
        "\n",
        "        # Emotion prediction\n",
        "        emotion_input = np.expand_dims(emotion_feat, axis=0)\n",
        "        emotion_input = np.expand_dims(emotion_input, axis=2)\n",
        "        emotion_pred = emotion_model.predict(emotion_input)\n",
        "        emotion_label = emotion_labels[np.argmax(emotion_pred)]\n",
        "        emotion_conf = np.max(emotion_pred)\n",
        "\n",
        "        return (\n",
        "            f\"✅ Detected Gender: Female (Confidence: {gender_prob:.2f})\\n\"\n",
        "            f\"🎭 Emotion: {emotion_label.capitalize()} (Confidence: {emotion_conf:.2f})\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error processing audio: {str(e)}\"\n",
        "\n",
        "# Gradio Interface\n",
        "interface = gr.Interface(\n",
        "    fn=predict_audio,\n",
        "    inputs=gr.Audio(type=\"filepath\", label=\"Upload or Record Voice\"),\n",
        "    outputs=gr.Textbox(label=\"Prediction\"),\n",
        "    title=\"🎙️ Emotion & Gender Detection (Female Voices Only)\",\n",
        "    description=\"Upload or record a voice clip. System detects gender first — only female voices are accepted for emotion prediction.\"\n",
        ")\n",
        "\n",
        "interface.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "Qir1QJC21TT9",
        "outputId": "5cdf08b5-f3a9-4bf7-ef8f-28da36a7d911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://ce35e3501f8758ef20.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://ce35e3501f8758ef20.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 422ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pECRwaNb1T1U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}